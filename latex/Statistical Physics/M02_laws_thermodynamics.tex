\documentclass{article}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref} % \url \href
\usepackage{docmute}

\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dbar}{\mathbf{\tilde{d}}}
\newcommand{\dnor}{\text{d}}
% \renewcommand{\H}{\mathcal{H}}

\begin{document}

\section{First Law}
Suppose we have the function $f(x)$, where $x$ are parameters of the state. 
If the integral:
\begin{equation}
    \Delta f = \int_{x_i}^{x_f} df = f(x_f) - f(x_i) 
\end{equation}
i.e., if $\Delta f$ is independent of the path chosen, then we call $f$ a 
\textbf{function of state}, which only depend on the system parameter at a 
certain instance.

\textbf{Equation of states} are equation that contain only function of states:
i.e., for ideal gas, we have $pV = RT$ which depend on $p, V, T$, all of them are 
function of states.

\textbf{First law}: energy is conserved in the form of heat and work.

Writting $U$ as the internal energy, The equation of states can be written as
\begin{gather}
    \Delta U = \Delta Q + \Delta W \\
    \dnor U = \dbar Q + \dbar W = \dbar Q - p \dnor V
\end{gather}
where $\Delta$ gives the total change and $\dnor$ represent differential change. $\dbar$
represent changes that are path dependent. It is clear that 
the heat absorbed and the work done to the system depend on the specific process, and 
thus they cannot be exact differentiated with respect to the system parameters, while 
volume $V$ is a function of states.

\subsection{Heat capacitiy of gas at constant volume or pressure}
For ideal gas, we have:
\begin{align}
    \dbar Q &= \dnor U + p \dnor V \\
    \dbar Q & = \left(\pfrac{U}{T}\right)_V \dnor T + \left(\pfrac{U}{V}\right)_T \dnor V + p \dnor V \\
    \frac{\dbar Q}{\dnor T} &= \left(\pfrac{U}{T}\right)_V + \left[ \left(\pfrac{U}{V}\right)_T  + p \right] \frac{\dnor V}{\dnor T}
\end{align}
Therefore, at fixed volume, $\dnor V = 0$ and we have:
\begin{equation}
    C_V = \left(\frac{\dbar Q}{\dnor T}\right)_V = \left(\pfrac{U}{T}\right)_V
\end{equation}
at fixed pressure:
\begin{align}
    C_p & = \left(\frac{\dbar Q}{\dnor T}\right)_p \\
        & = C_V + \left[ \left(\pfrac{U}{V}\right)_T  + p \right] \left(\frac{\dnor V}{\dnor T}\right)_p
\end{align}
For ideal gas, we have $\left(\pfrac{U}{V}\right)_T = 0$. and $p \left(\frac{\dnor V}{\dnor T}\right)_p = R$,
so that we have: $C_p = C_V + R$. For an equilibrium gas at constant pressure, the volume of the gas also expand when
the temperature of the gas is increased (gas do work to the environment). 
So we in generally need to input more heat to the increase the temperature of the gas, compared to the case of constant 
volume. 

In general gas, we define the ratio $\gamma$ called \textbf{adiabatic index}:
\begin{equation}
    \gamma = \frac{C_p}{C_V}
\end{equation}


\section{Second Law}

\subsection{Reversible process}
we define a process to be reversible if \textbf{at every moment during the process, the system
is in equilibrium}. As an example, for an ideal gas, if the equation of state $pV = RT$ is hold true
at every moment during the process, then it implies that the gas is always in an equilibrium state
and the process is reversible: we could not notice if this process happens in reversed. 
We consider two types of reversible process in terms of gas: First is called \textbf{Isothermal process}
and the second is called \textbf{Adiabatic process}.

In the isothermal process, the temperature of the system is fixed, in contact with a heat reservior:
\begin{equation}
    \Delta T = 0
\end{equation}
For ideal gas, this implies that $\Delta U = 0$, for reversible process of ideal gas, The 
equation of states $pV = RT$ is always true. This gives:
\begin{gather}
    \Delta U = \dbar Q - \frac{RT}{V} \dnor V = 0
\end{gather}
\[
\boxed{\Delta Q = \int_{V_1}^{V_2} \dbar Q = \int_{V_1}^{V_2} \frac{RT}{V} \dnor V = RT \ln \frac{V_2}{V_1}}
\]

In the adiabatic process, the system is not allowed to exchange heat with the 
environment, thus we have $\dbar Q = 0$. We find:
\begin{gather}
    \dnor U = \dbar W \\
    C_V dT = - p \dnor V = - \frac{RT}{V} \dnor V \\
    \ln \frac{T_2}{T_1} = - \frac{R}{C_V} \ln \frac{V_2}{V_1}
\end{gather}
For ideal gas, $C_p = C_V + R$, $\gamma = 1 + R / C_V$ and we find:
\begin{gather}
    \ln \frac{T_2}{T_1} = (1-\gamma) \ln \frac{V_2}{V_1} \\
    \frac{T_2}{T_1} = \left( \frac{V_2}{V_1} \right) ^ {1-\gamma} = \left( \frac{V_1}{V_2} \right) ^ {\gamma-1} \\
    T V^{\gamma - 1} = \text{const}
\end{gather}
\[ \boxed{p V^{\gamma} = \text{const}} \]

where we used relationship $pV = RT$ to obtain the last relationship. Equation $p V^{\gamma}$ 
enable us to find the pressure of the system if we know the volume of the 
system during a adiabatic expansion or compression. Temperature can be derived using the ideal 
gas law then, if we know the pressure and volume.

\subsection{Second law}
The second law can be stated in the following two equivalent form:

\textbf{Clausius statement} No process whose sole effect is to transfer heat from cold to hot body

\textbf{Kelvin statement} No process is possible whose sole effect is to convert heat into work

It is important to note the word "sole effect". For example, in an isothermal process of ideal gas, 
$\dbar Q + \dbar W = 0$ so all the heat is converted into work done by the system. However, the 
accompanying effect is that the volume of the gas expanded. 
Therefore, To study the ability to convert heat into work, we should consider process which has no 
other effect other than the conversion, such as Carnot engine which work in cycles

\subsection{Carnot engine}
We define a carnot engine which consist of two reversible adiabatic and two reversible 
isothermal process, working between temperature $T_h$ and $T_l$. The carnot engine do work
in the following process:

\begin{table*}[h]
    \centering
    \begin{tabular}{ccc}
        $ A \to B $ & isothermal expansion & $Q_h = RT_h \ln \frac{V_B}{V_A}$ \\
        $ B \to C $ & adiabatic & $ \frac{T_h}{T_l} = \left( \frac{V_C}{V_B} \right) ^ {\gamma-1} $ \\
        $ C \to D $ & isothermal compression & $Q_l = - RT_h \ln \frac{V_D}{V_C}$ \\
        $ D \to A $ & adiabatic & $ \frac{T_h}{T_l} = \left( \frac{V_D}{V_A} \right) ^ {\gamma-1}$ \\
    \end{tabular}
\end{table*}

The adiabatic process lead to $\frac{V_C}{V_B} = \frac{V_D}{V_A} $, with $V_B > V_A$. Finally, we 
find the important result for a carnot energy:
\[\boxed{\frac{Q_h}{T_h} = \frac{Q_l}{T_l} }\]
\begin{equation}
    Q_h - Q_l = W 
\end{equation}

where $W$ is the work done in one carnot cycle. We can find the efficiency of the engine
\footnote{This carnot engine seems to convert all the heat absorbed into work, $Q_h - Q_l \to W$. But
it has the additonal effect to transport heat from one source to another. Therefore, this does not 
conflict with Kelvin's statement}:
\begin{equation}
    \eta = \frac{W}{Q_h} = 1 - \frac{T_l}{T_h}
\end{equation}

We have the following statement related to the (reversible) carnot engines:
\begin{itemize}
    \item Carnot engine is the most efficient engine
    \item all reversible engine operating at the same temperature environment have the same efficiency
    \item Clausius statement of the second law is the same as Kelvin's statement \footnote{See book}
\end{itemize}

\subsection{Refrigerator}
We consider the above carnot energy run backwards: it absorb heat $Q_l$ from the 
low temperature side with an isothermal expansion. Input work $W$ is provided 
during the adiabatic process, and dump heat $Q_h$ into the the hot side 
with an isothermal compression process. The result for such a refrigerator is:
\begin{gather}
    Q_l + W = Q_h \\
    \frac{Q_h}{T_h} = \frac{Q_l}{T_l} \\
    \eta = \frac{Q_l}{W} = \frac{T_l}{T_h - T_l}
\end{gather} 

\subsection{Clausius theorem}
Suppose we have a seriers of heat exchange in a working cycle of an engine. At 
each step, the engine exchange heat $\dbar Q_i $ from a \textbf{contact} with 
temperature $T_i$. The maximum work this engine can produce is then:
\begin{equation}
    \Delta W = \sum_i \dbar Q_i
\end{equation}

Furthermore, suppose each of these contact are connect to \textbf{a single environment} 
with temperature $T$ through a carnot cycle. Each of these carnot cycle can be considered
as heat pumps or refrigerators, which operates to provide (absorb) heat $\dbar Q_i $
at each contact.
We have the relationship for these carnot cycles:
\begin{gather}
    \frac{\dbar Q}{T} = \frac{\dbar Q_i}{T_i} \\
    \dbar Q = \dbar Q_i + \dbar W_i 
\end{gather}

Since this system work in cycles, it cannot absorb heat from a single heat resevior (environment at $T$) 
and output work (second law). So the total work output by this system is necessary zero (do nothing) or negative 
(external work need to be provided to get this system going). So we have the following requirement:
\begin{equation}
    \sum_i \dbar W_i + \Delta W \le 0
\end{equation}
We have:
\begin{gather}
    \dbar W_i = \dbar Q - \dbar Q_i = \dbar Q_i (\frac{T}{T_i} - 1) \\
    \Delta W = \sum_i \dbar Q_i 
\end{gather}
So we can find:
\begin{gather}
    \sum_i \dbar Q_i \frac{T}{T_i} \le 0 
\end{gather}
\[
\boxed{\sum_i \frac{\dbar Q_i}{T_i} \le 0 \ \ \text{or}\ \  \oint \frac{\dbar Q}{T} \le 0}
\]
The second equation is the result of the Clausius theorem. The equal sign is only achieved for a reversible energy,
which we write:
\[
   \boxed{ \oint \frac{\dbar Q_{rev}}{T} = 0 }
\]

\subsection{Entropy}
For an reversible process, we have the relationship:
\begin{equation}
    \oint \frac{\dbar Q_{rev}}{T} = 0
\end{equation}
suggesting that the integral:
\begin{equation}
    \int_{A}^{B} \frac{\dbar Q_{rev}}{T} \ \ \text{is path independent}
\end{equation}
We therefore define a value $\dnor S = \dbar Q_{rev} / T$ and the value 
of $S$ is an function of state. $S$ is called the (Clausius) \textbf{entropy}. 
\textbf{For adiabatic process, $\dbar Q = 0$ and $\dnor S = 0$}.

In general, we consider a cycle consists of both reversible and 
irreversible process. According to Clausius theorem, we have:
\begin{equation}
    \oint \frac{\dbar Q}{T} = \int_A^B \frac{\dbar Q}{T} + \int_B^A \frac{\dbar Q_{rev}}{T} \le 0
\end{equation}
so that we have:
\begin{equation}
    \int_A^B \frac{\dbar Q}{T} \le \int_A^B \frac{\dbar Q_{rev}}{T} = \int_A^B \dnor S  \label{entropy_increase}
\end{equation}
taking the process from $A$ to $B$ infinitsemial, we thus find:
\begin{equation}
    \frac{\dbar Q}{T} \le \dnor S
\end{equation}
and the equality only happens for reversible process.

In thermally isolated system, $\dbar Q = 0$, and we reach an
important conclusion that for such isolated system, the entropy
of the system can only increase: $ \dnor S \ge 0$.

We take note that in this example, we entropy increase from process A to process B, 
as indicated by the integral in Eq.\ref{entropy_increase}. If we choose the direction
to be B $\to$ A, then we will have  $ \dnor S \le 0$. However, for a irreversible process,
the direction is fixed and only the direction that lead to an increase of entropy
will be naturally occuring (A $\to$ B). 

For a reversible process, $\dbar Q = T \dnor S$ and the first law gives:
\begin{align}
    \dnor U & = T \dnor S + \dbar W \\
            & = T \dnor S - p \dnor V \label{internal energy}
\end{align}
Since value $U, T, S, p, V$ are all function of state. This 
result is independent of the process and also holds for 
irreversible process: 
for an irreversible process, $\dnor U = \dbar Q + \dbar W$
but:
\begin{gather}
    \dbar Q \neq  T \dnor S \\
    \dbar W \neq  - p \dnor V 
\end{gather}
and Eq.\ref{internal energy} still holds
\begin{equation}
    \dbar Q + \dbar W = \dbar Q_{rev} - p \dnor V = T \dnor S - p \dnor V
\end{equation}

Equation.\ref{internal energy} also give the result:
\begin{gather}
    p = - \left(\frac{\dnor U}{\dnor V}\right)_S \\
    T = \left(\frac{\dnor U}{\dnor S}\right)_V
\end{gather}

\subsection{Joule Expansion}
We consider a process of Joule expansion as an example of irreversible process:
suppose a container is separate into half, each with volume $V_0$. The gas is 
initially confined one side of the container with pressure $p_i$. The other side
of the container is vacuum. Now we remove the separation and let the 
gas take up the volume of the whole container.

We have the equation of states:
\begin{gather}
    p_i V_0 = RT_i \\
    p_f 2V_0 = RT_f
\end{gather}
since the internal energy of the gas does not change, $\Delta U = 0$ and therefore 
for ideal gas, its temperature remain the same: $T_i = T_f$. We thus have:
$p_f = \frac{1}{2} p_i$. and
\begin{gather}
    \dnor U = T \dnor S - p \dnor V = 0 \\
    \dnor S = \frac{p}{T} \dnor V \\ 
    \Delta S = \int_i^f \dnor S = \int_i^f \frac{p}{T} \dnor V = \int_i^f \frac{R}{V} \dnor V = R \ln2
\end{gather}
where we consider the process as a reversible isothermal expansion, but is since $S$ is function
of state, the result is true for any process
\footnote{
Joule expansion is associated with Maxwell's Demon, which is an interesting read at page 149 of the book
}.

\subsection{Entropy of macrostates and total entropy}
Since we have the statistical definition of temperature
\begin{equation}
    \frac{1}{k_BT} = \frac{\dnor \ln\Omega}{\dnor E}
\end{equation}
where $\Omega$ is the number of microstates with energy $E$. 
Assume a microcanonical ensemble (each microstates have the same energy
and are equally probably), then the internal energy coinside 
with energy $E$. Using the relation:
\begin{equation}
    T = \left( \pfrac{U}{S} \right)_V
\end{equation}
We find the result:
\begin{equation}
    S = k_B \ln \Omega
\end{equation}
which we call the \textbf{Boltzmann's definition of entropy}.
\footnote{\url{https://physics.stackexchange.com/questions/141321/what-is-the-conceptual-difference-between-gibbs-and-boltzmann-entropies}}.
The entropy increase of the joule expansion can be obtained from the Boltzmann's entropy formula
by considering the number of phase space before and after the expansion: The phase space of 
velocity remain the same while the phase space of possible particle position is doubled. Giving the 
result: $\Delta S = R \ln2$.

The above case correspond to an isolated system with fixed energy (microcanonical ensemble). We now
consider the case where the system of interest is connected to a reservior. 
The number of states microstate of the whole system is $N$, We have:
\begin{equation}
    S_{tot} = S_{sys} + S_{res} = k_B \ln N
\end{equation}
suppose that the $i^{th}$ microstate of our system correspond to $n_i$ microstate of the reservior,
then the probability of our system to be in the $i^{th}$ microstate is then $P_i = n_i / N$
with corresponding entropy of the reservior $S_{res,i} = k_B \ln n_i$. The average entropy 
of the reservior can then be calculated:
\begin{equation}
    S_{res} = \langle S_{res} \rangle = \sum_i P_i S_{res,i} = \sum_i P_i k_B\ln n_i
\end{equation}
We have than the entropy of our system:
\begin{align}
    S & = k_B \left(\sum_i p_i\right) \ln N - \sum_i P_i k_B\ln n_i \\
    & = - k_B \sum_i p_i \ln p_i
\end{align}
where we used the result $\sum_i p_i = 1$. This is known as the \textbf{Gibb expression of entropy}.
The reasoning applied here is the same as finding the probability of an canonical system. 
The probability here can be given, for example, by canonical distribution function $p_i = e^{-\beta E_i} / Z$

Gibbs entropy formula is useful because most of the system we consider are connected to some 
heat bath at some tmperature $T$. Similar to why canonical ensemble is more useful than microcanonical 
ensemble. 
Also, Gibb's formula only contain probability, which can be measured, compared to number of microstates
in Boltzmann formula, which cannot be measured.
\footnote{
This is original expression given by the book, which seems to express a slightly different concept
related to the macroscopic properties. So I kept it here in the footnote. But I think the reasoning 
is the same.

Suppose the system 
have in total N possible microstates of equal probability, 
which can be divided into $i$ macrostates that we can distinguish by experimental measurement.
Each of those macrostates contain $n_i$ microstates. We define the Gibb's entropy:
\begin{equation}
    S_{tot} = S + S_{micro}
\end{equation}
where $ S_{tot} = k_B \ln N$. The microscopic entropy is given by:
\begin{equation}
    S_{micro} = \langle S \rangle = \sum_i P_i S_i = \sum_i P_i k_B\ln n_i
\end{equation}
where the probability $p_i = n_i / N$ and $\sum_i p_i = 1$. We have:
\begin{align}
    S & = k_B \left(\sum_i p_i\right) \ln N - \sum_i P_i k_B\ln n_i \\
    & = - k_B \sum_i p_i \ln p_i
\end{align}
}

\subsection{Entropy of information}
In information theory, we define the information content $Q$ of 
a statement (or a prediction) by:
\begin{equation}
    Q = - k \log P
\end{equation}
where $P$ is the probability that the statement turn out to be true, and
$k$ is a positive scaling constants. If we take $k = 1$, and take $\log_2$ for logarithm, then
the information content is measured in bits. 

As an example, consider the 
following statement: \emph{a single coin toss give a result of "face up"}. The 
information content is:
\begin{equation}
    Q = - \log_2 P = 1
\end{equation}
Therefore, this information is 1 bit long: to store the above statement, we only require 1 bit
to store whether the result is face up with ${0, 1}$.

Now, to describe(transmit) a probability distribution, we can use(transmit) a series of statements. For example, 
to describe the probability distribution of weather (sunny, cloudy and rainy) in a day, we 
have different events with different probabilities and information contents:
\begin{itemize}
    \item the probability of a rainy day is $20\%$
    \item the probability of a sunny day is $40\%$
    \item the probability of a cloudy day is $40\%$
\end{itemize}
We associate each statement with its information content and define the \textbf{average information content}
as:
\begin{equation}
    S = \langle Q \rangle = -k \sum_i P_i \log P_i
\end{equation}
information content $S$ is called \textbf{Shannon entropy}. Shannon entropy therefore give the average information
content of succesive measurements.

As an example, consider a Bernoulli trial with outcome probability $p$. Using $k=1$, the 
Shannon entropy is calculated by:
\begin{equation}
    S = - \sum_i P_i \log_2 P_i = -p \log_2 p - (1-p) \log_2 (1-p)
\end{equation}
$S$ is maximum when $p = 0.5$, showing that the information content of a Bernoulli distribution is greatest
when the uncertainty of output is greatest.
When $p$ deviates from $0.5$, the outcome become biased and it is very likely that most measurement
give the same result. The information content with this probability distribution decrease. In the extreme case, if the event is 
deterministic with single outcome, than we will gain no information after measurement of the result.

\subsection{Entropy and encoding}
The Shannon entropy measures the average information content, which can be interpreted as the \textbf{average bits needed} to
transit the measurement, when measured in bits. Consider the following two case:

\textbf{Case 1.} Suppose an event have 8 possible outcomes with equal probability, 
the Shannon entropy is then:
\begin{equation}
    S = - 8 \times \frac{1}{8} \log_2\frac{1}{8} = 3\ bits
\end{equation}
the distribution is random, so we need to use 3 bits to store each outcome and there is no way to compress the informations.

\textbf{Case 2.} Suppose now that we have different probability of the outcome given by the following distribution:
\begin{equation*}
    {1/2, 1/4, 1/8, 1/16, 1/64, 1/64, 1/64, 1/64}
\end{equation*}
Now, the Shannon entropy is:
\begin{equation}
    S = - \sum_i P_i \log_2 P_i = 2\ bits
\end{equation}
It is now possible to compress the result by the following code: $0, 10, 110, 1110, 111100, 111101, 111110, 111111$ and the average length
of the code we need to transmit is:
\begin{equation}
    L = 1/2 \times 1 + 1/4 \times 2 + 1/8 \times 3 + 1/16 \times 4 + 4 \times 1/64 \times 6= 3\ bits
\end{equation}
which agree with the value of the Shannon entropy where the length required to store one result is given by $Q = - \log_2 P$.


\section{Third Law}
We can measure the change of entropy of a system by measuring its heat capacitiy:
\begin{gather}
    C_p = T\left(\pfrac{S}{T}\right)_p \label{thirdlawcp}\\
    S = \int \frac{C_p}{T} \dnor T
\end{gather}
so that we have:
\begin{equation}
    S(T) = S(T_0) + \int_{T_0}^{T} \frac{C_p}{T} \dnor T
\end{equation}

The third law states that the entropy of a system at absolute zero will be zero:

\textbf{Planck's statement of the third law} The entropy of all systems in internal
equilibrium is the same at absolute zero and may be taken to be zero.

It is noted that the system should be in a relaxed internal equilibrium for the third law
to hold, it is possible,
for example, to freeze a metastable glass phase instead of a crystalline phase and go down
o $T \to 0$ with non zero entropy.

As a consequence of the third law, Heat capacitiy, given by Eq.\ref{thirdlawcp} will
necessarily go to zero as $T\to0$.

As another implication, Using the statistical definition of the entropy $S = k_B \ln \Omega$
implies that the ground states of a system at absolute zero, is necessary non-degenerate with
$\Omega = 1$ so that the entropy will be zero. Such non-degeneracy would be enforced by the 
third law for real systems, as well as the requirement for $C_pto 0$. 
As an example, the curie's law gives a susceptibility $\chi \to \infty$ as $T \to 0$. But with
the third law, we require $\partial \chi / \partial T \to 0$. This problem is caused because
for Curie law, we assumed a mean field interaction where the interaction of magnetic
moment is ignored\footnote{See discussion in page 204-205}. 


\end{document}


