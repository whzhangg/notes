\documentclass{article}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref} % \url \href
\usepackage{docmute}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\spn}{span}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newtheorem*{theorem}{Theorem}
\newtheorem*{definition}{Definition}
% \renewcommand{\H}{\mathcal{H}}

\begin{document}

\title{Solution to Linear System of Equations}
\author{Wenhao}
\date{\today}
\maketitle

\section{Direct methods}

\subsection{Substitution methods}
For a non-singular lower triangular system, such as:
\begin{equation*}
    \left(\begin{matrix}
        l_{11} & 0 & 0 \\
        l_{21} & l_{22} & 0 \\
        l_{31} & l_{32} & l_{33} 
    \end{matrix}\right)
    \left(\begin{matrix}
        x_1 \\ x_2 \\ x_3
    \end{matrix}\right) = 
    \left(\begin{matrix}
        b_1 \\ b_2 \\ b_3
    \end{matrix}\right)
\end{equation*}
the solution can be obtained as:
\begin{align*}
    x_1 &= b_1 / l_{11} \\
    x_2 &= (b_2 - l_{21} x_1 ) / l_{22} \\
    x_3 &= (b_3 - l_{31} x_1 - l_{32} x_2) / l_{33}
\end{align*}
Such algorithm can be extended to $n\times n$ systems 
$\mathbf{L}\mathbf{x} = \mathbf{b}$ ($\mathbf{L}$ for lower)
and is called \emph{forward substitution}
and it takes the general form:
\begin{equation}
    \label{forward_substitution}
    x_1 = \frac{b_1}{l_{11}} \qquad
    x_i = \frac{1}{l_{ii}} \left(b_i - \sum_{j = 1}^{i-1} l_{ij}x_j\right) 
\end{equation}
for $i = 2, \dots, n$. The number of multiplication needed is $1, \dots, n$ for each 
equation and the number of substraction needed is $0, \dots, n-1$. Therefore, the total 
number of operation needed is $n^2$. 

For upper triangular matrix ($\mathbf{U}\mathbf{x} = \mathbf{b}$), we can use similar procedure but now called 
\emph{backward substitution} method.

\subsection{Gaussian elimination method}
The \emph{Gaussian elimiation} (GE) method reduce the linear equation 
$\mathbf{A}\mathbf{x} = \mathbf{b}$ to an equivalent equation 
of the form $\mathbf{U}\mathbf{x} = \tilde{\mathbf{b}}$ where $\tilde{\mathbf{b}}$
is the updated right hand side. 
This is done by eliminating the leading columns to zero by adding appropriately multiplied 
previous row to the current row, for example:
\begin{align*}
    \left(\begin{matrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22} 
    \end{matrix}\right)
    \left(\begin{matrix}
        x_1 \\ x_2
    \end{matrix}\right) &= 
    \left(\begin{matrix}
        b_1 \\ b_2
    \end{matrix}\right) \\
    \left(\begin{matrix}
        a_{11} & a_{12} \\
        0 & a_{22} - \frac{a_{21}}{a_{11}} a_{12}
    \end{matrix}\right)
    \left(\begin{matrix}
        x_1 \\ x_2
    \end{matrix}\right) &= 
    \left(\begin{matrix}
        b_1 \\ b_2 - \frac{a_{21}}{a_{11}} b_1
    \end{matrix}\right)
\end{align*}
Formally, we denote $\mathbf{A}^{(k)}$ and $\mathbf{b}^{(k)}$ as updates of the 
original matrix $\mathbf{A}$ and $\mathbf{b}$. Assuming that $a_{kk}^{(k)}\neq 0$ and 
we can define the multiplier:
\begin{equation*}
    m_{ik} = \frac{a_{ik}^{(k)}}{a_{kk}^{(k)}} 
    ,\qquad i = k+1, \dots, n
\end{equation*}
so that the updates can be written as
\begin{align}
    a_{ij}^{(k+1)} &= a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)}
    ,\qquad i,j = k+1, \dots, n \notag \\
    b_{i}^{(k+1)} &= b_{i}^{(k)} - m_{ik}a_{k}^{(k)}
    ,\qquad i = k+1, \dots, n \\
\end{align}
the term $a_{kk}^{(k)}$ is called pivots. Matrix $\mathbf{A}$ may have zero-valued pivots so 
that the elimination process is interrupted following the above form. In this case, a 
permutation can be applied to the matrix (Pivoting technique Section 3.5).

Gaussian elmiination process is equivalent to performing a matrix factorization of $\mathbf{A}$
into the product of two matrices, lower and upper triangular matrix:
\begin{equation}
    \label{LU}
    mathbf{A} = \mathbf{L} \mathbf{U}
\end{equation}
in a similar process as in GE. 
Once the matrices $\mathbf{L}$ and $\mathbf{U}$ have been computed, we can solve the linear 
system as:
\begin{align*}
    \mathbf{L}\mathbf{y} = \mathbf{b} \\
    \mathbf{U}\mathbf{x} = \mathbf{y}
\end{align*}
i.e., by solving two triangular system.

\subsection{$\mathbf{L} \mathbf{D} \mathbf{M}^T$ factorization}
There are other factorization method to solve the linear equation.
A square matrix $\mathbf{A}$ can be factorized in the form of:
\begin{equation}
    \mathbf{A} = \mathbf{L} \mathbf{D} \mathbf{M}^T
\end{equation}
where $\mathbf{L}, \mathbf{D}, \mathbf{M}^T$ are lower trianglular, diagonal and 
upper diagonal matrix. The solution of this system can be carried out by 
\begin{align*}
    \mathbf{L}  \mathbf{y} = \mathbf{b} \\
    \mathbf{D}  \mathbf{z} = \mathbf{y} \\
    \mathbf{M}^T\mathbf{x} = \mathbf{z} \\
\end{align*}

\subsection{Cholesky factorization}
If $\mathbf{A}$ is symmetric and positive definit, then there exists a unique 
\emph{upper triangular} matrix $\mathbf{H}$ with positive diagonal entries so 
that 
\begin{equation}
    \mathbf{A} = \mathbf{H}^T \mathbf{H}
\end{equation}
This is called \emph{Cholesky factorization} and the entries can be computed as:
\begin{align*}
    h_{11} &= \sqrt{a_{11}} \\
    h_{ij} &= \left( a_{ij} - \sum_{k=1}^{j-1} h_{ik}h_{jk} \right) / h_{jj}, \qquad j = 1,\dots,i-1 \\
    h_{ii} &= \left( a_{ii} - \sum_{k=1}^{i-1} h_{ik}^2 \right)^{1/2}
\end{align*}
and the solution of the linear system can be obtained by substitution method.

\subsection{Undetermined systems}
When matrix $\mathbf{A}^{m\times n}$ is not a square matrix, we say that 
the solution of $\mathbf{A}\mathbf{x} = \mathbf{b}$ is \emph{overdetermined} if $m>n$ 
or \emph{underdetermined} if $m < n$. 
If a system is underdetermined, there is generally no solution unless 
$\mathbf{b}\in \range(\mathbf{A})$

If a system is overdetermined $m \leq n$, we say that $\mathbf{x}^*$ is a solution
of the linear system $\mathbf{A}\mathbf{x} = \mathbf{b}$ \emph{in the least squares sense}
if 
\begin{equation}
    \Phi(\mathbf{x}^*) = \|\mathbf{A}\mathbf{x}^* - \mathbf{b}\|_2^2 
    \leq \min_{\mathbf{x}} \|\mathbf{A}\mathbf{x} - \mathbf{b}\|_2^2 
    = \min_{\mathbf{x}} \Phi(\mathbf{x})
\end{equation}
i.e, the $\mathbf{x}^*$ closet to satisfy the linear system. Now the problem 
become a minimization problem and can be found by:
\begin{gather*}
    \Phi(\mathbf{x}) 
    = (\mathbf{A}\mathbf{x} - \mathbf{b})^T (\mathbf{A}\mathbf{x} - \mathbf{b})
    = \mathbf{x}^T \mathbf{A}^T \mathbf{A} \mathbf{x} - 2 \mathbf{x}^T \mathbf{A}^T \mathbf{b} 
      +  \mathbf{b}^T \mathbf{b} \\
    \nabla \Phi(\mathbf{x}^*) 
    = 2 \mathbf{A}^T \mathbf{A} \mathbf{x}^* - 2 \mathbf{A}^T \mathbf{b} = 0 \\
    \mathbf{A}^T \mathbf{A} \mathbf{x}^* = \mathbf{A}^T \mathbf{b}
\end{gather*}
where the last equation is known as \emph{normal equations}. If $\mathbf{A}$ is full rank, the 
least-squares solution exists and is unique. 
Since $\mathbf{B} = \mathbf{A}^T \mathbf{A}$ is symmetry and positive definite, we can use 
Cholesky factorization to solve the normal equation to obtain the solution
\footnote{another method is QR factorization, which is not covered here}.

If $\mathbf{A}$ is not full rank, than the solution is not unique, since if $\mathbf{x}^*$ 
is a solution, than $\mathbf{x}^* + \mathbf{z}$ is also a solution. To make solution unique,
we need to introduce a further constraint. Typically, we can require that $\mathbf{x}^*$
has a minimal euclidean norm $\|\mathbf{x}^*\|$.
This is consistent with the full rank case where only one unique solution that has the 
minimal euclidean norm. 

\subsection{Solution using SVD}
The above problem with the minimal norm requirement can be solved using SVD.
Let $\mathbf{A}\in \mathbb{R}^{m\times n}$ with SVD given by 
$\mathbf{A} = \mathbf{U}\Sigma \mathbf{V}^T$, then the unique solution is 
given by
\begin{equation}
    \mathbf{x}^* = \tilde{\mathbf{A}} \mathbf{b}
\end{equation}
where $\tilde{\mathbf{A}}$ is the \emph{Pseudo-inverse} of $\mathbf{A}$.

\end{document}