\documentclass{article}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref} % \url \href

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\spn}{span}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newtheorem*{theorem}{Theorem}
\newtheorem*{definition}{Definition}
% \renewcommand{\H}{\mathcal{H}}

\begin{document}

\title{Solution to Linear System of Equations}
\author{Wenhao}
\date{\today}
\maketitle

\section{Basic concepts}
A system of $m$ linear equations with $n$ unknowns consists of a set of alegbraic relations
\begin{equation*}
    \sum_{j=1}^n a_{ij}x_j = b_i,\qquad i = 1, \dots, m
\end{equation*}
or in matrix form:
\begin{equation}
    \label{matrixform}
    \mathbf{A}\mathbf{x} = \mathbf{b}
\end{equation}
If $\mathbf{A}\in \mathbb{R}^{n\times n}$ and $\mathbf{b}\in \mathbb{R}^{n}$, the 
existance and uniqueness of the solution is ensured if the any of the following equivalent 
condition holds:
\begin{enumerate}
    \item $\mathbf{A}$ is invertible
    \item $\rank(A) = n$
    \item $\mathbf{A}\mathbf{x} =0$ only has null solution
\end{enumerate}

\section{Cramer's rule}
The solution of the linear equation can be formally obtained using \emph{Cramer's rule}:
\begin{equation}
    \label{cramer_rule}
    x_j = \frac{\Delta_j}{\det(A)},\qquad j = 1, \dots, m  
\end{equation}
where $\Delta_j$ is the determinant of the matrix obtained by substituting the $j$-th 
column of $\mathbf{A}$ with the right hand side $\mathbf{b}$.
\footnote{\url{https://www.math.purdue.edu/~shao92/documents/Proof_Cramer_rule.pdf} provide a short proof}

However, the evaluation of the determinant is computational expensive (summing over all 
permutations) and impractice for application. Numerical methods that are alternative to 
Cramer's rule can be classified into two types:
they are called \emph{direct} if they yield solution of the system in a finite number of 
steps, and \emph{iterative} if they require, in principle, infinite number of steps.

\section{Direct methods}
\subsection{Substitution methods}
For a non-singular lower triangular system, such as:
\begin{equation*}
    \left(\begin{matrix}
        l_{11} & 0 & 0 \\
        l_{21} & l_{22} & 0 \\
        l_{31} & l_{32} & l_{33} 
    \end{matrix}\right)
    \left(\begin{matrix}
        x_1 \\ x_2 \\ x_3
    \end{matrix}\right) = 
    \left(\begin{matrix}
        b_1 \\ b_2 \\ b_3
    \end{matrix}\right)
\end{equation*}
the solution can be obtained as:
\begin{align*}
    x_1 &= b_1 / l_{11} \\
    x_2 &= (b_2 - l_{21} x_1 ) / l_{22} \\
    x_3 &= (b_3 - l_{31} x_1 - l_{32} x_2) / l_{33}
\end{align*}
Such algorithm can be extended to $n\times n$ systems 
$\mathbf{L}\mathbf{x} = \mathbf{b}$ ($\mathbf{L}$ for lower)
and is called \emph{forward substitution}
and it takes the general form:
\begin{equation}
    \label{forward_substitution}
    x_1 = \frac{b_1}{l_{11}} \qquad
    x_i = \frac{1}{l_{ii}} \left(b_i - \sum_{j = 1}^{i-1} l_{ij}x_j\right) 
\end{equation}
for $i = 2, \dots, n$. The number of multiplication needed is $1, \dots, n$ for each 
equation and the number of substraction needed is $0, \dots, n-1$. Therefore, the total 
number of operation needed is $n^2$. 

For upper triangular matrix ($\mathbf{U}\mathbf{x} = \mathbf{b}$), we can use similar procedure but now called 
\emph{backward substitution} method.

\subsection{Gaussian elimination method}
The \emph{Gaussian elimiation} (GE) method reduce the linear equation 
$\mathbf{A}\mathbf{x} = \mathbf{b}$ to an equivalent equation 
of the form $\mathbf{U}\mathbf{x} = \tilde{\mathbf{b}}$ where $\tilde{\mathbf{b}}$
is the updated right hand side. 
This is done by eliminating the leading columns to zero by adding appropriately multiplied 
previous row to the current row, for example:
\begin{align*}
    \left(\begin{matrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22} 
    \end{matrix}\right)
    \left(\begin{matrix}
        x_1 \\ x_2
    \end{matrix}\right) &= 
    \left(\begin{matrix}
        b_1 \\ b_2
    \end{matrix}\right) \\
    \left(\begin{matrix}
        a_{11} & a_{12} \\
        0 & a_{22} - \frac{a_{21}}{a_{11}} a_{12}
    \end{matrix}\right)
    \left(\begin{matrix}
        x_1 \\ x_2
    \end{matrix}\right) &= 
    \left(\begin{matrix}
        b_1 \\ b_2 - \frac{a_{21}}{a_{11}} b_1
    \end{matrix}\right)
\end{align*}
Formally, we denote $\mathbf{A}^{(k)}$ and $\mathbf{b}^{(k)}$ as updates of the 
original matrix $\mathbf{A}$ and $\mathbf{b}$. Assuming that $a_{kk}^{(k)}\neq 0$ and 
we can define the multiplier:
\begin{equation*}
    m_{ik} = \frac{a_{ik}^{(k)}}{a_{kk}^{(k)}} 
    ,\qquad i = k+1, \dots, n
\end{equation*}
so that the updates can be written as
\begin{align}
    a_{ij}^{(k+1)} &= a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)}
    ,\qquad i,j = k+1, \dots, n \notag \\
    b_{i}^{(k+1)} &= b_{i}^{(k)} - m_{ik}a_{k}^{(k)}
    ,\qquad i = k+1, \dots, n \\
\end{align}
the term $a_{kk}^{(k)}$ is called pivots. Matrix $\mathbf{A}$ may have zero-valued pivots so 
that the elimination process is interrupted following the above form. In this case, a 
permutation can be applied to the matrix (Pivoting technique Section 3.5).

Gaussian elmiination process is equivalent to performing a matrix factorization of $\mathbf{A}$
into the product of two matrices, lower and upper triangular matrix:
\begin{equation}
    \label{LU}
    mathbf{A} = \mathbf{L} \mathbf{U}
\end{equation}
in a similar process as in GE. 
Once the matrices $\mathbf{L}$ and $\mathbf{U}$ have been computed, we can solve the linear 
system as:
\begin{align*}
    \mathbf{L}\mathbf{y} = \mathbf{b} \\
    \mathbf{U}\mathbf{x} = \mathbf{y}
\end{align*}
i.e., by solving two triangular system.

\subsection{$\mathbf{L} \mathbf{D} \mathbf{M}^T$ factorization}
There are other factorization method to solve the linear equation.
A square matrix $\mathbf{A}$ can be factorized in the form of:
\begin{equation}
    \mathbf{A} = \mathbf{L} \mathbf{D} \mathbf{M}^T
\end{equation}
where $\mathbf{L}, \mathbf{D}, \mathbf{M}^T$ are lower trianglular, diagonal and 
upper diagonal matrix. The solution of this system can be carried out by 
\begin{align*}
    \mathbf{L}  \mathbf{y} = \mathbf{b} \\
    \mathbf{D}  \mathbf{z} = \mathbf{y} \\
    \mathbf{M}^T\mathbf{x} = \mathbf{z} \\
\end{align*}

\subsection{Cholesky factorization}
If $\mathbf{A}$ is symmetric and positive definit, then there exists a unique 
\emph{upper triangular} matrix $\mathbf{H}$ with positive diagonal entries so 
that 
\begin{equation}
    \mathbf{A} = \mathbf{H}^T \mathbf{H}
\end{equation}
This is called \emph{Cholesky factorization} and the entries can be computed as:
\begin{align*}
    h_{11} &= \sqrt{a_{11}} \\
    h_{ij} &= \left( a_{ij} - \sum_{k=1}^{j-1} h_{ik}h_{jk} \right) / h_{jj}, \qquad j = 1,\dots,i-1 \\
    h_{ii} &= \left( a_{ii} - \sum_{k=1}^{i-1} h_{ik}^2 \right)^{1/2}
\end{align*}
and the solution of the linear system can be obtained by substitution method.

\subsection{Undetermined systems}
When matrix $\mathbf{A}^{m\times n}$ is not a square matrix, we say that 
the solution of $\mathbf{A}\mathbf{x} = \mathbf{b}$ is \emph{overdetermined} if $m>n$ 
or \emph{underdetermined} if $m < n$. 
If a system is underdetermined, there is generally no solution unless 
$\mathbf{b}\in \range(\mathbf{A})$

If a system is overdetermined $m \leq n$, we say that $\mathbf{x}^*$ is a solution
of the linear system $\mathbf{A}\mathbf{x} = \mathbf{b}$ \emph{in the least squares sense}
if 
\begin{equation}
    \Phi(\mathbf{x}^*) = \|\mathbf{A}\mathbf{x}^* - \mathbf{b}\|_2^2 
    \leq \min_{\mathbf{x}} \|\mathbf{A}\mathbf{x} - \mathbf{b}\|_2^2 
    = \min_{\mathbf{x}} \Phi(\mathbf{x})
\end{equation}
i.e, the $\mathbf{x}^*$ closet to satisfy the linear system. Now the problem 
become a minimization problem and can be found by:
\begin{gather*}
    \Phi(\mathbf{x}) 
    = (\mathbf{A}\mathbf{x} - \mathbf{b})^T (\mathbf{A}\mathbf{x} - \mathbf{b})
    = \mathbf{x}^T \mathbf{A}^T \mathbf{A} \mathbf{x} - 2 \mathbf{x}^T \mathbf{A}^T \mathbf{b} 
      +  \mathbf{b}^T \mathbf{b} \\
    \nabla \Phi(\mathbf{x}^*) 
    = 2 \mathbf{A}^T \mathbf{A} \mathbf{x}^* - 2 \mathbf{A}^T \mathbf{b} = 0 \\
    \mathbf{A}^T \mathbf{A} \mathbf{x}^* = \mathbf{A}^T \mathbf{b}
\end{gather*}
where the last equation is known as \emph{normal equations}. If $\mathbf{A}$ is full rank, the 
least-squares solution exists and is unique. 
Since $\mathbf{B} = \mathbf{A}^T \mathbf{A}$ is symmetry and positive definite, we can use 
Cholesky factorization to solve the normal equation to obtain the solution
\footnote{another method is QR factorization, which is not covered here}.

If $\mathbf{A}$ is not full rank, than the solution is not unique, since if $\mathbf{x}^*$ 
is a solution, than $\mathbf{x}^* + \mathbf{z}$ is also a solution. To make solution unique,
we need to introduce a further constraint. Typically, we can require that $\mathbf{x}^*$
has a minimal euclidean norm $\|\mathbf{x}^*\|$.
This is consistent with the full rank case where only one unique solution that has the 
minimal euclidean norm. 

\subsection{Solution using SVD}
The above problem with the minimal norm requirement can be solved using SVD.
Let $\mathbf{A}\in \mathbb{R}^{m\times n}$ with SVD given by 
$\mathbf{A} = \mathbf{U}\Sigma \mathbf{V}^T$, then the unique solution is 
given by
\begin{equation}
    \mathbf{x}^* = \tilde{\mathbf{A}} \mathbf{b}
\end{equation}
where $\tilde{\mathbf{A}}$ is the \emph{Pseudo-inverse} of $\mathbf{A}$.



\newpage
\section{Iterative solution}
\subsection{General form of iterative solution}
The basic idea of iterative methods is to construct a sequence of vectors 
$\mathbf{x}^{(k)}$:
\begin{equation*}
    \mathbf{x} = \lim_{k\to \infty} \mathbf{x}^{(k)}
\end{equation*}
In the most general form, we can write an iteration method as:
\begin{align}
    \mathbf{x}^{(0)} &= f_0(\mathbf{A}, \mathbf{b}) \notag \\
    \mathbf{x}^{(n+1)} &= f_{n+1}(\mathbf{x}^{(n)}, \mathbf{x}^{(n-1)}, \dots,  
    \mathbf{x}^{(n-m)}, \mathbf{A}, \mathbf{b})
\end{align}
where $f$ are functions that take the problem $(\mathbf{A}, \mathbf{b})$, as well as results of the previous 
$m$ iterations. We call $m$ the \emph{order of the method}. 

A method is called \emph{stationary} if the function $f_i$ is independent of the step index $i$, 
otherwise it is called \emph{nonstationary}. If the function $f$ depend linearly on previous steps 
$\mathbf{x}^{(n)}, \mathbf{x}^{(n-1)}, \dots,  \mathbf{x}^{(n-m)}$, it is called a 
\emph{linear} method. Otherwise it is called \emph{nonlinear}. 

\subsection{Stationary linear method}
\begin{definition}
let $\|\cdot\|$ denote a norm on $\mathbb{R}^N$ vector space,
we call the induced matrix 
norm of a square matrix $\mathbf{A}$:
\begin{equation*}
    \|\mathbf{A}\|=\max_{\|x\|=1} \|\mathbf{A}\mathbf{x}\|
\end{equation*}
\end{definition}

The induces matrix norm has the property
\begin{equation*}
    \|\mathbf{A}\mathbf{x}\| \leq \|\mathbf{A}\| \|\mathbf{x}\|
\end{equation*}

\begin{definition}
    We define the condition number of a matrix $\mathbf{A}$ as:
    \[\kappa(\mathbf{A}) = \|\mathbf{A}\| \|\mathbf{A^{-1}}\|\] 
\end{definition}
Condition number determines the relationship between 
\emph{error}: $\mathbf{e} = \mathbf{x} - \mathbf{x}^*$ and 
\emph{residual}: $\mathbf{r} = \mathbf{b} - \mathbf{A}\mathbf{x}^*$
by the following results:
\begin{equation}
    \frac{\|\mathbf{e}\|}{\|\mathbf{e}_0\|} \leq \kappa(\mathbf{A}) \frac{\|\mathbf{r}\|}{\|\mathbf{r}_0\|}
\end{equation}
As a result, termination criteria based on residual gives a bound for the error. One common and straightforward 
termination criteria is therefore:
\begin{equation*}
    \frac{\|\mathbf{r}_k\|}{\|\mathbf{r}_0\|} = \frac{\|\mathbf{r}_k\|}{\|\mathbf{b}\|} < \epsilon
\end{equation*}

\begin{definition}
    We define the spectral radius of $\mathbf{A}$ as:
    \[ \rho(\mathbf{A}) = \max_{\lambda} |\lambda| = \lim_{n\to\infty} \|\mathbf{A}^n\|^{1/n} \] 
\end{definition}
where $\lambda$ is the eigenvalue of the matrix. 
The spectral radius has the following property 
for any choice of norm ($p$):
\footnote{we can verify, for a vector $\|v\|_1 \geq \|v\|_2$ and $\|v\|_{n} \geq \|v\|_{n+1}$ (proof?). 
The result $\rho(\mathbf{A}) \leq \|\mathbf{A}\|_p$ for any $p$ therefore come from its definition}
\begin{equation*}
    \rho(\mathbf{A}) \leq \|\mathbf{A}\|_p    
\end{equation*}

\subsubsection*{General form of stationary linear iteration}
We consider the iterative methods of the form 
\begin{equation}
    \label{generalstationarylinear}
    \mathbf{x}^{(k+1)} = \mathbf{B}\mathbf{x}^{(k)} + \mathbf{f}
\end{equation}
with $\mathbf{x}^{(0)}$ (initial guess) given. $\mathbf{B}$ is called the iteration matrix.
An iterative method is called \emph{consistent} if 
$\mathbf{x} = \mathbf{B}\mathbf{x} + \mathbf{f}$. Consistency does not necessary lead
to convergence. 

\begin{theorem}
    For a consistent iterative method, the sequence of $x^{(k)}$ converge to the solution 
    of the linear system for any choice of $\mathbf{x}^{(0)}$ if and only if 
    $\rho(\mathbf{B}) < 1$. $\rho(\mathbf{B})$ is the spectral radius defined above.
\end{theorem}

A consistent iterative equation can be derived from $\mathbf{A}\mathbf{x} = \mathbf{b}$ by 
splitting $\mathbf{A} = \mathbf{I} - \mathbf{B}$:
\begin{gather*}
    (\mathbf{I} - \mathbf{B}) \mathbf{x} = \mathbf{b} \qquad \mathbf{x} = \mathbf{B} \mathbf{x} + \mathbf{b}
\end{gather*}
and letting $\mathbf{x}$ at both side of the equation to be different.

\subsection{Methods based on splitting the matrix (preconditioning)}
The general technique of linear iterative method is to split the matrix $\mathbf{A}$
into form 
\begin{equation}
    \mathbf{A} = \mathbf{P} - \mathbf{N}
\end{equation}
and $\mathbf{P}$ is non-singular (invertible). We call $\mathbf{P}$ the 
\emph{preconditioning matrix}. We update the solution by:
\begin{equation*}
    \mathbf{P}\mathbf{x}^{(k+1)} = \mathbf{N}\mathbf{x}^{(k)} + \mathbf{b}  
\end{equation*}
We can verify that the above equation can be equivalently written as:
\begin{equation}
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \mathbf{P}^{-1}\mathbf{r}^{(k)}
\end{equation}
where $\mathbf{r}^{k} = \mathbf{b} - \mathbf{A}\mathbf{x}^{(k)}$, is 
the \emph{residual vector} at step $k$.
We call this form the \emph{stationary Richardson method}.
The above iterative method can be written as 
\begin{align*}
    \mathbf{x}^{(k+1)} &= \mathbf{x}^{(k)} + \mathbf{P}^{-1}\mathbf{r}^{(k)} \\
    &= \mathbf{x}^{(k)} + \mathbf{P}^{-1} \left(\mathbf{b} - \mathbf{A}\mathbf{x}^{(k)} \right) \\
    &= \left( \mathbf{I} - \mathbf{P}^{-1}\mathbf{A}  \right) \mathbf{x}^{(k)} + \mathbf{P}^{-1} \mathbf{b} 
\end{align*}
and the iteration matrix is therefore $\mathbf{I} - \mathbf{P}^{-1}\mathbf{A} = \mathbf{R_p}$
\footnote{this form is the same as equation 1.12 by Kelley 1995}.

\begin{definition}
    $\mathbf{B}$ is called an approximate inverse of $\mathbf{A}$ if 
    $\|\mathbf{I} - \mathbf{A}\mathbf{B}\| < 1$
\end{definition}
The iteration in the form of 
$\mathbf{x}^{(k+1)} = \left( \mathbf{I} - \mathbf{P}^{-1}\mathbf{A}  \right) \mathbf{x}^{(k)} + \mathbf{P}^{-1} \mathbf{b} $
converge much faster if the norm of $\mathbf{I} - \mathbf{P}^{-1}\mathbf{A}$ is small, 
i.e., $\mathbf{P}^{-1}$ is an approximate inverse of $\mathbf{A}$ (or $\mathbf{P}$ is approximately $\mathbf{A}$). 
In fact, if $\mathbf{I} - \mathbf{P}^{-1}\mathbf{A} = 0$,
we have, immediately, the exact solution:
\begin{equation*}
    \mathbf{x}^{(k+1)} = \mathbf{A}^{-1} \mathbf{b}
\end{equation*}
$\mathbf{P}$ should also be chosen to be easily invertible,
A common choice of $\mathbf{P}$ can be the the diagonal preconditioner
\footnote{see section 4.3.2}. 

\subsubsection{Jacobi method}
If the diagonal entries of matrix $\mathbf{A}$ are non-zero, we can rewrite the 
original linear equations as:
\begin{equation*}
    x_i = \frac{1}{a_{ii}}\left[
        b_i - \sum_{j=1, j\neq i}^n a_{ij}x_j
    \right], \quad i = 1, \dots, n
\end{equation*}
The \emph{Jacobi method} updates the solution according to this form:
\begin{equation}
    x_i^{(k+1)} = \frac{1}{a_{ii}}\left[
        b_i - \sum_{j=1, j\neq i}^n a_{ij}x_j^{(k)}
    \right], \quad i = 1, \dots, n
\end{equation}
The Jacobi method can be equivalently formulated as 
$\mathbf{A} = \mathbf{D} - \mathbf{N}$ where $\mathbf{D}$ is a diagonal part of matrix $\mathbf{A}$.

\subsubsection{Over-relaxation}
Based on Jacobi method, we introduce a relaxation parameter $\omega$, so that 
the iteration is now:
\begin{equation}
    x_i^{(k+1)} = \frac{\omega}{a_{ii}}\left[
        b_i - \sum_{j=1, j\neq i}^n a_{ij}x_j^{(k)}
    \right] + (1-\omega)x_j^{(k)}
\end{equation}
The introduction of the relaxation parameter $\omega$ in many case 
helps the convergence.

\subsubsection{Gauss-Seidel method}
The \emph{Gauss-Seidel method} differ from the Jacobi method in that the $(k+1)$
step uses the available values of $x_i^{(k+1)}$ to update the solution, so that 
we have:
\begin{equation}
    x_i^{(k+1)} = \frac{1}{a_{ii}}\left[
        b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)} - \sum_{j = i+1}^n a_{ij}x_j^{(k)}
    \right], \quad i = 1, \dots, n
\end{equation}
which is equivalent to splitting $\mathbf{A}$ as 
\begin{equation*}
    \mathbf{A} = (\mathbf{D}-\mathbf{E}) - \mathbf{F}  
\end{equation*}
where $\mathbf{E}$ is a 
lower triangular matrix and $\mathbf{F}$ is a upper triangular matrix without
diagonal components. 
The relaxation parameter can also be applied to the Gauss-Seidel method.

\subsection{Non-stationary Richardson method}
In \emph{Nonstationary Richardson method}, we let
\begin{equation}
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{P}^{-1}\mathbf{r}^{(k)}
\end{equation}
where $\alpha_k$ is a number depend on the current state and is 
called \emph{acceleration parameter}. Computationally, the solution can be computed 
by the following steps:
\begin{enumerate}
    \item We first find $\mathbf{z}^{k} = \mathbf{P}^{-1}\mathbf{r}^{(k)}$
    \item compute the acceleration parameter $\alpha_k$
    \item update the solution $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{z}^{k}$
    \item update the residual $\mathbf{r}^{(k+1)} = \mathbf{r}^{(k)} - \alpha_k \mathbf{A} \mathbf{z}^{k}$ 
\end{enumerate}
The following gradient based methods are examples of the non-stationary richardson method.

\subsection{Gradient method}
In the case that $\mathbf{A}$ is symmetric and positive definite, we can use gradient
to compute the optimal acceleration parameter. This is called the 
\emph{gradient method}. 
Solution to the linear equation $\mathbf{A}\mathbf{x} = \mathbf{b}$ is equivalent 
to finding the minimizer $\mathbf{x}$ of the \emph{energy of system}
\begin{equation}
    \Phi(\mathbf{y}) 
    = \frac{1}{2}\mathbf{y}^T \mathbf{A} \mathbf{y} - \mathbf{y}^T \mathbf{b}
\end{equation}
We can verify that the gradient of energy is given by the expression
\begin{equation*}
    \nabla \Phi(\mathbf{y}) = \frac{1}{2} (\mathbf{A}^T + \mathbf{A}) \mathbf{y} - \mathbf{b}
     = \mathbf{A} \mathbf{y} - \mathbf{b}
\end{equation*}
which is 0 when the solution is found, and
\begin{equation*}
    \Phi(\mathbf{y}) = \Phi(\mathbf{x} + (\mathbf{y} - \mathbf{x}))
    = \Phi(\mathbf{x}) + \frac{1}{2} (\mathbf{y} - \mathbf{x}) ^T \mathbf{A} (\mathbf{y} - \mathbf{x})
\end{equation*}
we call $(\mathbf{y} - \mathbf{x}) ^T \mathbf{A} (\mathbf{y} - \mathbf{x})$ 
the $\mathbf{A}$-norm: 
\begin{equation*}
    \|\mathbf{y} - \mathbf{x}\|^2_{\mathbf{A}} 
    = (\mathbf{y} - \mathbf{x}) ^T \mathbf{A} (\mathbf{y} - \mathbf{x})
\end{equation*} 
The gradient of the energy norm at iteration $\mathbf{x}^{(k)}$ is found to be:
\begin{equation*}
    \nabla \Phi(\mathbf{x}^{(k)}) = \mathbf{A} \mathbf{x}^{(k)} - \mathbf{b} = -\mathbf{r}^{(k)}
\end{equation*}
Therefore, the residual give the direction of the gradient of $\Phi$ and the 
update should follow the opposite direction of the gradient:
\begin{equation}
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)}
\end{equation}
where the acceleration parameter $\alpha_k$ gives the distance. 
To obtain a value for $\alpha_k$, 
we hope that the updated solution $\mathbf{x}^{(k+1)}$ lie approximately at the minimum. So we 
differentiate
\begin{equation*}
    \Phi(\mathbf{x}^{(k+1)}) 
    = \frac{1}{2}(\mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)})^T 
    \mathbf{A} (\mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)}) 
    - (\mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)})^T \mathbf{b}
\end{equation*}
with respect to $\alpha_k$ and letting it equal 0. The result for $\alpha_k$ is:
\begin{equation}
    \label{alpha_k}
    \alpha_k = 
    \frac{\left.\mathbf{r}^{(k)}\right.^T \mathbf{r}^{(k)}}
    {\left.\mathbf{r}^{(k)}\right.^T \mathbf{A} \mathbf{r}^{(k)}}
\end{equation}

In summary, the computation procedure for gradient method is:
\begin{enumerate}
    \item given $\mathbf{x}^{(0)}$, we compute the residual 
        $\mathbf{r}^{(k)} = \mathbf{b} - \mathbf{A}\mathbf{x}^{(k)}$
    \item compute the acceleration parameter according to Equation \eqref{alpha_k}
    \item update the result using $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)}$
\end{enumerate}

\subsection{Conjugate gradient method}
Although gradient method follow the gradient at each point of the solution $\mathbf{x}^{(k)}$,
the solution path may not be the most optimal. In conjugate gradient method, we try to find 
the optimal path of updates with:
\begin{equation}
    \label{cg_update}
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)}
\end{equation}
with $\alpha_k$ still given by minimizing the energy of system with respect to $\alpha$:
\begin{equation}
    \alpha_k = 
    \frac{\left.\mathbf{p}^{(k)}\right.^T \mathbf{r}^{(k)}}
    {\left.\mathbf{p}^{(k)}\right.^T \mathbf{A} \mathbf{p}^{(k)}}
\end{equation}
But now we want the updates to be on optmial steps, so we have to determine the 
direction $\mathbf{p}^{(k)}$.
\begin{definition}
    $\mathbf{x}^{(k)}$ is said to be \emph{optimal} with respect to a direction $\mathbf{p}\neq 0$
    if 
    \[\Phi(\mathbf{x}^{(k)})\leq \Phi(\mathbf{x}^{(k)} + \lambda \mathbf{p})\]
    for any $\lambda$. If $\mathbf{x}^{(k)}$ is optimal with respect to any direction in a vector 
    space $V$, then we call $\mathbf{x}^{(k)}$ \emph{optimal with respect to} V
\end{definition}
For $\mathbf{x}^{(k)}$ to be optimal with respect to $\mathbf{p}$, it is necessary that 
the residual (gradient) $\mathbf{r}^{(k)}$ is perpendicular to the direction $\mathbf{p}$,
otherwise, we can always descend along the projection of the gradient along $\mathbf{p}$
to a position with lower energy.

Suppose $\mathbf{x}^{(k)}$ is optimal with respect to $\mathbf{p}$, so that 
$\mathbf{p}^T\mathbf{r}^{(k)} = 0$ and we update by 
\begin{equation*}
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \mathbf{q}  
\end{equation*}
If we require that $\mathbf{x}^{(k+1)}$ is still optimal to $\mathbf{p}$, then 
we have:
\begin{align*}
    \mathbf{p}^T \mathbf{r}^{(k+1)} 
    = \mathbf{p}^T \left( \mathbf{b} - \mathbf{A}(\mathbf{x}^{(k)} + \mathbf{q}) \right)
    = \mathbf{p}^T \left( \mathbf{r}^{(k)} - \mathbf{A}\mathbf{q} \right)
    = - \mathbf{p}^T \mathbf{A}\mathbf{q} = 0
\end{align*}
Therefore, the descent direction $\mathbf{q}$ must be \emph{$\mathbf{A}$-orthogonal} to 
$\mathbf{p}$. 

The process of conjugate gradient method start by choosing $\mathbf{x}^{(0)}$ and 
obtaining $\mathbf{r}^{(0)}$. We let the initial direction $\mathbf{p}^{(0)}$ to be 
\[\mathbf{p}^{(0)} = \mathbf{r}^{(0)}\]
and update the solution using equation \eqref{cg_update}
($\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)}$),
where $\alpha_k$ is chosen so that $\mathbf{x}^{(k+1)}$ \emph{is} optimal to the 
direction $\mathbf{p}^{(k)}$, i.e., $\left.\mathbf{r}^{(k+1)}\right.^T \mathbf{p}^{(k)} = 0$.
After the next $\mathbf{x}^{(k+1)}$ is chosen, we evalulate the residual $\mathbf{r}^{(k+1)}$
and update the direction so that
\begin{equation}
    \mathbf{p}^{(k+1)} = \mathbf{r}^{(k+1)} - \beta_k \mathbf{p}^{(k)}
\end{equation}
where $\beta$ is determined so that $\mathbf{p}^{(k+1)}$ and $\mathbf{p}^{(k)}$
is $\mathbf{A}$-orthogonal:
\begin{gather}
    \left(\mathbf{A} \mathbf{p}^{(k)} \right)^T \mathbf{p}^{(k+1)} = 0 \notag\\
    \beta_k = \frac{\left(\mathbf{A} \mathbf{p}^{(k)} \right)^T \mathbf{r}^{(k+1)}}{\left(\mathbf{A} \mathbf{p}^{(k)} \right)^T \mathbf{p}^{(k)}}
\end{gather}
so that $\mathbf{x}^{(k+2)}$ is still optimal with respect to $\mathbf{p}^{(k)}$. 
In summary, 
we note that the parameter $\alpha_k$ ensures that the updated $\mathbf{x}^{(k+1)}$
is optimal to direction $\mathbf{p}^{(k)}$, while the choice $\beta_k$ ensures 
that the $\mathbf{p}^{(k)}$ and $\mathbf{p}^{(k-1)}$ are $\mathbf{A}$-orthogonal
and $\mathbf{x}^{(k+1)}$ is optimal to previous direction $\mathbf{p}^{(k-1)}$.

The updates of the conjugate gradient method can be summarized by the above equation:
\begin{align*}
    & \alpha_k = 
    \frac{\left.\mathbf{p}^{(k)}\right.^T \mathbf{r}^{(k)}}
    {\left.\mathbf{p}^{(k)}\right.^T \mathbf{A} \mathbf{p}^{(k)}} \\
    & \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \\
    & \mathbf{r}^{(k+1)} = \mathbf{r}^{(k)} - \alpha_k \mathbf{A}\mathbf{p}^{(k)} \\
    & \beta_k = \frac{\left(\mathbf{A} \mathbf{p}^{(k)} \right)^T \mathbf{r}^{(k+1)}}{\left(\mathbf{A} \mathbf{p}^{(k)} \right)^T \mathbf{p}^{(k)}} \\
    & \mathbf{p}^{(k+1)} = \mathbf{r}^{(k+1)} - \beta_k \mathbf{p}^{(k)}
\end{align*}
The two parameters $\alpha_k$ and $\beta_k$ can be alternatively expressed as
\begin{equation*}
    \alpha_k = 
    \frac{\|\mathbf{r}^{(k)}\|_2^2}
    {\left.\mathbf{p}^{(k)}\right.^T \mathbf{A} \mathbf{p}^{(k)}}, \qquad 
    \beta_k = \frac{ \|\mathbf{r}^{(k+1)}\|_2^2 }{ \|\mathbf{r}^{(k)}\|_2^2} 
\end{equation*}
and Kelley 1995 Section 2.4 provide an pseudo-code of implementation and a detailed discussion 
of termination and preconditioning.

We can show that in fact, using this update strategy, $\mathbf{p}^{(k+1)}$ is $\mathbf{A}$-orthogonal
to all the previous $\mathbf{p}^{(j)}$ for $j = 0,1,\dots,k$.
This can be proven by induction: 
$\mathbf{p}^{(1)}$ is by design $\mathbf{A}$-orthogonal to $\mathbf{p}^{(0)}$. 
If the directions $\mathbf{p}^{(0)}, \dots, \mathbf{p}^{(k-1)}$ are mutually $\mathbf{A}$-orthogonal
and $\mathbf{x}^{(k)}$ is optimal to all these directions, we have, for 
$j = 0, 1, \dots, k-1$:
\begin{align*}
    \left(\mathbf{A} \mathbf{p}^{(j)} \right)^T \mathbf{p}^{(k+1)} 
    &= \left(\mathbf{A} \mathbf{p}^{(j)} \right)^T 
            \left( \mathbf{r}^{(k+1)} - \beta_k \mathbf{p}^{(k)} \right) \\ 
    &= \left(\mathbf{A} \mathbf{p}^{(j)} \right)^T \mathbf{r}^{(k+1)} 
\end{align*}
On the other hand, we have:
\begin{align*}
    \left(\mathbf{p}^{(j)} \right)^T \mathbf{r}^{(k+1)} 
    &= \left(\mathbf{p}^{(j)} \right)^T 
        \left( \mathbf{r}^{(k)} - \alpha_k \mathbf{A}\mathbf{p}^{(k)}\right)  \\
    &= \left(\mathbf{p}^{(j)} \right)^T \mathbf{r}^{(k)} 
       - \alpha_k  \left(\mathbf{p}^{(j)} \right)^T \mathbf{A}\mathbf{p}^{(k)} = 0
\end{align*}
for all $j = 0, 1, \dots, k-1$ because of the assumption of optimal and mutual $\mathbf{A}$-orthogonality.
$\left.\mathbf{p}^{(j)} \right.^T \mathbf{r}^{(k+1)} = 0$ shows that $\mathbf{r}^{(k+1)}$ is 
orthogonal to all the vectors $\mathbf{p}^{(j)}$ and therefore to the vector space:
\begin{equation}
    V_k = \spn(\mathbf{p}^{(0)}, \dots, \mathbf{p}^{(k-1)})
\end{equation} 
Since we update $\mathbf{p}$ by:
\begin{align*}
    & \mathbf{p}^{(0)} = \mathbf{r}^{(0)} \\
    & \mathbf{r}^{(k+1)} = \mathbf{r}^{(k)} - \alpha_k \mathbf{A}\mathbf{p}^{(k)} \\
    & \mathbf{p}^{(k+1)} = \mathbf{r}^{(k+1)} - \beta_k \mathbf{p}^{(k)}
\end{align*}
we can see that $\spn(\mathbf{p}^{(0)}, \dots, \mathbf{p}^{(k-1)})$ is equal to 
$\spn(\mathbf{r}^{(0)}, \dots, \mathbf{r}^{(k-1)})$ and $\mathbf{A}\mathbf{p}^{(j)} \in V_{j+1}$.
As a result, if $\mathbf{r}^{(k+1)}$ is orthogonal to $V_k$, 
$\mathbf{r}^{(k+1)}$ is also orthogonal to all the vectors $\mathbf{A} \mathbf{p}^{(j)}$:
\begin{equation*}
    \left(\mathbf{A} \mathbf{p}^{(j)} \right)^T \mathbf{r}^{(k+1)} = 0
\end{equation*}
which proves that 
\begin{equation*}
    \left(\mathbf{A} \mathbf{p}^{(j)} \right)^T \mathbf{p}^{(k+1)} = 0
\end{equation*}
i.e., if $\mathbf{p}^{(0)}, \dots, \mathbf{p}^{(k-1)}$ are mutually $\mathbf{A}$-orthogonal
and $\mathbf{x}^{(k)}$ is optimal to all these directions, $\mathbf{p}^{(k)}$ is also mutually
$\mathbf{A}$-orthogonal and $\mathbf{x}^{(k+1)}$ is optmial to all these directions
$\mathbf{p}^{(0)}, \dots, \mathbf{p}^{(k-1)}$.

Conjugate gradient method gives a optimal path to obtain the solution to the linear equation.
Regarding conjugate gradient method, we have the following theorem 
\begin{theorem}
If $\mathbf{A}$ is a symmetric and positive definite matrix, any method that use 
conjugate directions to find solution terminate at most $n$ steps, yielding the exact solution.    
\end{theorem}

\subsection{Methods based on Krylov Subspace}
Starting from Richardson method (i.e., we do not require symmetric or positive definit matrix), 
we have the generalization of iteration method:
\begin{align*}
    \mathbf{x}^{(k+1)} &= \mathbf{x}^{(k)} + \alpha_k \mathbf{P}^{-1}\mathbf{r}^{(k)} \\
    \mathbf{r}^{(k+1)} &= \mathbf{r}^{(k)} - \alpha_k \mathbf{A} \mathbf{P}^{-1}\mathbf{r}^{(k)}
\end{align*}
letting $\mathbf{P} = \mathbf{I}$, we have, for the residual:
\begin{equation}
    \mathbf{r}^{(k)} = \prod_{j=0}^{k-1} \left(\mathbf{I} - \alpha_j \mathbf{A}\right) \mathbf{r}^{(0)} 
    = p_k(\mathbf{A}) \mathbf{r}^{(0)}
\end{equation}
where $p_k(\mathbf{A})$ is the polynomial in $\mathbf{A}$ of degree $k$. Similarly, the iteration 
in Richardson method is given by:
\begin{equation}
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(0)} + \sum_{j=0}^{k-1} \alpha_j \mathbf{r}^{(j)}
\end{equation}
Define the \emph{Krylov subspace}
$K_m(\mathbf{A},\mathbf{v}) = \spn( \mathbf{v}, \mathbf{A}\mathbf{v}, \dots, \mathbf{A}^{m-1}\mathbf{v} )$,
we have $p_{k-1}(\mathbf{A}) \mathbf{v} \in K_m(\mathbf{A},\mathbf{v})$ for any polynomial in $\mathbf{A}$.
So that 
\begin{align*}
    \mathbf{r}^{(k+1)} &\in K_{k+1}(\mathbf{A},\mathbf{r}^{(0)}) \\
    \mathbf{x}^{(k)} &\in \{\mathbf{v}\mid \mathbf{v} = \mathbf{x}^{(0)} + \mathbf{y}, \mathbf{y} \in K_{k}(\mathbf{A},\mathbf{r}^{(0)})\}
\end{align*}
and we denote the vector space:
\begin{equation*}
    W_k = \{\mathbf{v}\mid \mathbf{v} = \mathbf{x}^{(0)} + \mathbf{y}, \mathbf{y} \in K_{k}(\mathbf{A},\mathbf{r}^{(0)})\}
\end{equation*}
so that $\mathbf{x}^{(k)} \in W_k$

Therefore, generally, we can search for an solution of $\mathbf{x}^{(k)}$ in the form:
\begin{equation}
    \mathbf{x}^{(k)} = \mathbf{x}^{(0)} + p_{k-1}(\mathbf{A}) \mathbf{r}^{(0)}
\end{equation}
Methods that look for solution in this form is known as the \emph{Krylov method}. 
Conjugate gradient method is therefore a form of hte Krylov method.
Two approach can be used to search for a candidate for $\mathbf{x}^{(k)}$:
\begin{enumerate}
    \item The first method computes $\mathbf{x}^{(k)}\in W_k$ by enforcing that the residual $\mathbf{r}^{(k)}$
    to be orthogonal to any vector in $K_{k}(\mathbf{A},\mathbf{r}^{(0)})$, i.e, 
    \begin{equation}
        \mathbf{v}^T \left( \mathbf{b} - \mathbf{A} \mathbf{x}^{(k)} \right) = 0
        ,\quad \text{for all}\ \mathbf{v} \in K_{k}
    \end{equation}
    This method is known as the \emph{Arnoldi} method (or full orthogonalization method) for linear system.
    \item The second method require that $\mathbf{x}^{(k)}$ to be the 
    vector that minimize the euclidean norm of the residual:
    \begin{equation}
        \|\mathbf{b} - \mathbf{A} \mathbf{x}^{(k)}\|_2 
        = \min_{v\in W_k} \|\mathbf{b} - \mathbf{A} \mathbf{v}\|_2
    \end{equation}
    This method is known as the generalized minimum residual method (GMRES). 
\end{enumerate}
GMRES method is described in detail in the book of Kelley 1995. 
Both method should converge after finite steps.

\end{document}